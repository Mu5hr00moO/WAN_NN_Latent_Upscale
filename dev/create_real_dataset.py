import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
import os
from tqdm import tqdm
import requests
import zipfile
from pathlib import Path
import json

class RealLatentDatasetCreator:
    """
    Creates a proper dataset using real images for latent upscaling training.
    Based on COCO 2017 validation images, center cropped to 512x512.
    """
    
    def __init__(self, vae_path="models/vae/wan_vae.safetensors", output_dir="datasets/real_latent"):
        self.vae_path = vae_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Load VAE for encoding
        self.vae = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def download_coco_val2017(self):
        """Download COCO 2017 validation dataset."""
        coco_dir = Path("datasets/coco2017")
        coco_dir.mkdir(parents=True, exist_ok=True)
        
        images_dir = coco_dir / "val2017"
        if images_dir.exists() and len(list(images_dir.glob("*.jpg"))) > 100:
            print(f"COCO validation images already exist: {len(list(images_dir.glob('*.jpg')))} images")
            return images_dir
            
        print("Downloading COCO 2017 validation images...")
        url = "http://images.cocodataset.org/zips/val2017.zip"
        zip_path = coco_dir / "val2017.zip"
        
        # Download with progress bar
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        
        with open(zip_path, 'wb') as f, tqdm(
            desc="Downloading COCO val2017",
            total=total_size,
            unit='B',
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    pbar.update(len(chunk))
        
        # Extract
        print("Extracting COCO images...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(coco_dir)
        
        # Clean up
        zip_path.unlink()
        
        return images_dir
    
    def load_vae(self):
        """Load VAE for encoding images to latents."""
        try:
            # Try to load from ComfyUI
            import sys
            sys.path.append("../..")

            import comfy.model_management as model_management
            import comfy.utils
            from comfy.sd import VAE

            # Find VAE file
            vae_paths = [
                "../../models/vae/wan_vae.safetensors",
                "../../models/vae/vae-ft-mse-840000-ema-pruned.safetensors",
                "../../models/vae/sdxl_vae.safetensors",
                "../../models/vae/ae.safetensors"
            ]

            vae_path = None
            for path in vae_paths:
                if os.path.exists(path):
                    vae_path = path
                    break

            if not vae_path:
                print("No VAE found. Creating synthetic latent dataset instead...")
                return self.create_synthetic_dataset()

            # Load VAE
            vae_data = comfy.utils.load_torch_file(vae_path)
            self.vae = VAE(sd=vae_data)
            self.vae = self.vae.to(self.device)
            print(f"Loaded VAE from {vae_path}")

        except Exception as e:
            print(f"Failed to load ComfyUI VAE: {e}")
            print("Creating synthetic latent dataset instead...")
            return self.create_synthetic_dataset()

        return True

    def create_synthetic_dataset(self):
        """Create a better synthetic dataset with more realistic patterns."""
        print("Creating improved synthetic latent dataset...")

        num_samples = 2000
        hr_latents = []
        lr_latents = []

        for i in tqdm(range(num_samples), desc="Creating synthetic latents"):
            # Create more realistic latent patterns
            # HR: 64x64 latent
            hr_latent = torch.randn(1, 16, 64, 64) * 0.5

            # Add some structure (simulating encoded image features)
            # Low frequency components
            hr_latent[:, :8, :, :] += torch.randn(1, 8, 64, 64) * 0.3
            # High frequency components
            hr_latent[:, 8:, :, :] += torch.randn(1, 8, 64, 64) * 0.1

            # LR: 32x32 latent (downsampled)
            lr_latent = F.interpolate(hr_latent, size=(32, 32), mode='bilinear', align_corners=False)
            # Add some noise to make it more realistic
            lr_latent += torch.randn_like(lr_latent) * 0.05

            hr_latents.append(hr_latent)
            lr_latents.append(lr_latent)

        # Save dataset
        dataset = {
            'hr_latents': torch.cat(hr_latents, dim=0),
            'lr_latents': torch.cat(lr_latents, dim=0),
            'metadata': {
                'num_samples': num_samples,
                'hr_resolution': '64x64 latent (synthetic)',
                'lr_resolution': '32x32 latent (synthetic)',
                'type': 'improved_synthetic',
                'source': 'Generated with realistic patterns'
            }
        }

        dataset_path = self.output_dir / "improved_synthetic_dataset.pt"
        torch.save(dataset, dataset_path)

        print(f"\nâœ… Improved synthetic dataset created!")
        print(f"ğŸ“ Saved to: {dataset_path}")
        print(f"ğŸ“Š Samples: {num_samples}")
        print(f"ğŸ“ HR Latents: {dataset['hr_latents'].shape}")
        print(f"ğŸ“ LR Latents: {dataset['lr_latents'].shape}")

        return True
    
    def center_crop_and_resize(self, image, target_size=512):
        """Center crop and resize image to target size."""
        # Convert to PIL if needed
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # Get dimensions
        width, height = image.size
        
        # Center crop to square
        min_dim = min(width, height)
        left = (width - min_dim) // 2
        top = (height - min_dim) // 2
        right = left + min_dim
        bottom = top + min_dim
        
        image = image.crop((left, top, right, bottom))
        
        # Resize to target size
        image = image.resize((target_size, target_size), Image.Resampling.LANCZOS)
        
        return image
    
    def image_to_tensor(self, image):
        """Convert PIL image to tensor."""
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Convert to tensor [0, 1]
        tensor = torch.from_numpy(np.array(image)).float() / 255.0
        
        # Rearrange to [C, H, W]
        tensor = tensor.permute(2, 0, 1)
        
        # Normalize to [-1, 1] for VAE
        tensor = tensor * 2.0 - 1.0
        
        return tensor
    
    def create_dataset_pairs(self, images_dir, num_samples=1000):
        """Create high-res and low-res latent pairs."""
        if not self.load_vae():
            return False
        
        image_files = list(images_dir.glob("*.jpg"))[:num_samples]
        print(f"Processing {len(image_files)} images...")
        
        hr_latents = []
        lr_latents = []
        
        for img_path in tqdm(image_files, desc="Creating latent pairs"):
            try:
                # Load and preprocess image
                image = Image.open(img_path)
                
                # High resolution: 512x512
                hr_image = self.center_crop_and_resize(image, 512)
                hr_tensor = self.image_to_tensor(hr_image).unsqueeze(0).to(self.device)
                
                # Low resolution: 256x256
                lr_image = self.center_crop_and_resize(image, 256)
                lr_tensor = self.image_to_tensor(lr_image).unsqueeze(0).to(self.device)
                
                # Encode to latents
                with torch.no_grad():
                    hr_latent = self.vae.encode(hr_tensor)  # Should be ~64x64 latent
                    lr_latent = self.vae.encode(lr_tensor)  # Should be ~32x32 latent
                
                # Store
                hr_latents.append(hr_latent.cpu())
                lr_latents.append(lr_latent.cpu())
                
                # Free memory
                del hr_tensor, lr_tensor, hr_latent, lr_latent
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
                continue
        
        # Save dataset
        dataset = {
            'hr_latents': torch.cat(hr_latents, dim=0),
            'lr_latents': torch.cat(lr_latents, dim=0),
            'metadata': {
                'num_samples': len(hr_latents),
                'hr_resolution': '512x512 -> ~64x64 latent',
                'lr_resolution': '256x256 -> ~32x32 latent',
                'vae_path': str(self.vae_path),
                'source': 'COCO 2017 validation'
            }
        }
        
        dataset_path = self.output_dir / "real_latent_dataset.pt"
        torch.save(dataset, dataset_path)
        
        print(f"\nâœ… Dataset created successfully!")
        print(f"ğŸ“ Saved to: {dataset_path}")
        print(f"ğŸ“Š Samples: {len(hr_latents)}")
        print(f"ğŸ“ HR Latents: {dataset['hr_latents'].shape}")
        print(f"ğŸ“ LR Latents: {dataset['lr_latents'].shape}")
        
        return True

def main():
    """Create real latent dataset for training."""
    creator = RealLatentDatasetCreator()
    
    # Download COCO dataset
    images_dir = creator.download_coco_val2017()
    
    # Create latent pairs
    success = creator.create_dataset_pairs(images_dir, num_samples=2000)
    
    if success:
        print("\nğŸ‰ Real dataset creation completed!")
        print("Now you can retrain the model with:")
        print("python retrain_with_real_data.py")
    else:
        print("\nâŒ Dataset creation failed!")

if __name__ == "__main__":
    main()
